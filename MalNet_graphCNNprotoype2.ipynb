{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58ee29de-2293-4444-9d8a-de4f2ba00ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: networkx\n",
      "Successfully installed networkx-3.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8b8dd6-a99f-4c6f-9014-58999a456c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/miniconda3/envs/tf_env/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp311-cp311-macosx_14_0_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-macosx_10_9_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.2-cp311-cp311-macosx_14_0_x86_64.whl (25.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.1/25.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3acb07b-49d2-4479-9958-a8907a33381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "import json\n",
    "\n",
    "def load_graphs_from_edgelist(data_dir, label_dir):\n",
    "    graphs, labels = [], []\n",
    "    \n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith(\".edgelist\") or file == \".DS_Store\":\n",
    "                continue\n",
    "            \n",
    "            graph_path = os.path.join(root, file)\n",
    "            rel_path = os.path.relpath(graph_path, data_dir)\n",
    "            path_parts = rel_path.split(os.sep)\n",
    "            \n",
    "            if len(path_parts) < 2:\n",
    "                print(f\"Skipping invalid path: {graph_path}\")\n",
    "                continue\n",
    "            \n",
    "            category = path_parts[0]\n",
    "            sha256 = os.path.splitext(file)[0]\n",
    "            label = 0 if category.lower() == 'benign' else 1\n",
    "            \n",
    "            try:\n",
    "                graph = nx.read_edgelist(graph_path, nodetype=int, encoding=\"utf-8\")\n",
    "                graphs.append(graph)\n",
    "                labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {graph_path}: {e}\")\n",
    "    \n",
    "    return graphs, np.array(labels)\n",
    "\n",
    "# Convert Graphs to Adjacency Matrices\n",
    "def graph_to_adj_matrix(graph, max_nodes=100):\n",
    "    adj_matrix = np.zeros((max_nodes, max_nodes))\n",
    "    nodes = list(graph.nodes())[:max_nodes]\n",
    "    node_map = {node: i for i, node in enumerate(nodes)}\n",
    "    \n",
    "    for edge in graph.edges():\n",
    "        if edge[0] in node_map and edge[1] in node_map:\n",
    "            i, j = node_map[edge[0]], node_map[edge[1]]\n",
    "            adj_matrix[i, j] = 1\n",
    "            adj_matrix[j, i] = 1  # Ensure symmetry\n",
    "    \n",
    "    return adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdffdd88-2ffe-4249-96b5-508ca05ae674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['addisplay', '.DS_Store', 'trojan', 'downloader', 'benign', 'all-less-than-5k-nodes.txt', 'adware']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"/Users/danigeorge/Documents/HPE Project/malnet-graphs-tiny\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81e2edfd-5df1-4126-87a9-df1953a9836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create Training and Testing Data\n",
    "def create_datasets(graphs, labels, test_size=0.2, max_nodes=100):\n",
    "    data = np.array([graph_to_adj_matrix(g, max_nodes) for g in graphs])\n",
    "    data = np.expand_dims(data, axis=-1)  # Add channel dimension\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return train_test_split(data, labels, test_size=test_size, random_state=42)\n",
    "\n",
    "# Custom callback to compute additional metrics per epoch\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        val_predictions = (self.model.predict(self.validation_data[0]) > 0.5).astype(int)\n",
    "        val_labels = self.validation_data[1]\n",
    "        logs['val_f1'] = f1_score(val_labels, val_predictions)\n",
    "        logs['val_auc'] = roc_auc_score(val_labels, val_predictions)\n",
    "        logs['val_precision'] = precision_score(val_labels, val_predictions)\n",
    "        logs['val_recall'] = recall_score(val_labels, val_predictions)\n",
    "        print(f\"Epoch {epoch+1}: Val F1: {logs['val_f1']:.4f}, Val AUC: {logs['val_auc']:.4f}, Val Precision: {logs['val_precision']:.4f}, Val Recall: {logs['val_recall']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c930278-8970-4ddd-aca1-c5f559ee0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define a CNN Model with Dropout and Early Stopping\n",
    "def build_cnn_model(input_shape):\n",
    "    model = keras.Sequential([\n",
    "        layers.Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dfa14a8-d4c0-4e61-8f33-a3cd6ca2c169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 1s/step - accuracy: 0.7763 - auc: 0.7244 - loss: 0.4558 - precision: 0.8066 - recall: 0.9428 - val_accuracy: 0.8260 - val_auc: 0.8518 - val_loss: 0.3759 - val_precision: 0.8348 - val_recall: 0.9722\n",
      "Epoch 2/35\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 1s/step - accuracy: 0.8552 - auc: 0.9010 - loss: 0.2975 - precision: 0.8723 - recall: 0.9635 - val_accuracy: 0.8330 - val_auc: 0.8631 - val_loss: 0.3545 - val_precision: 0.8480 - val_recall: 0.9608\n",
      "Epoch 3/35\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 952ms/step - accuracy: 0.9122 - auc: 0.9634 - loss: 0.2073 - precision: 0.9203 - recall: 0.9744 - val_accuracy: 0.8420 - val_auc: 0.8646 - val_loss: 0.4058 - val_precision: 0.8527 - val_recall: 0.9671\n",
      "Epoch 4/35\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 1s/step - accuracy: 0.9479 - auc: 0.9867 - loss: 0.1234 - precision: 0.9571 - recall: 0.9786 - val_accuracy: 0.8500 - val_auc: 0.8697 - val_loss: 0.3963 - val_precision: 0.8783 - val_recall: 0.9405\n",
      "Epoch 5/35\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 946ms/step - accuracy: 0.9720 - auc: 0.9908 - loss: 0.0873 - precision: 0.9756 - recall: 0.9902 - val_accuracy: 0.8450 - val_auc: 0.8640 - val_loss: 0.4178 - val_precision: 0.8705 - val_recall: 0.9443\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 112ms/step\n",
      "Validation Accuracy: 0.8330\n",
      "Validation F1 Score: 0.9009\n",
      "Validation AUC: 0.8632\n",
      "Validation Precision: 0.8480\n",
      "Validation Recall: 0.9608\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train and Evaluate the Model\n",
    "def train_model(train_data, train_labels, test_data, test_labels, epochs=35, batch_size=32):\n",
    "    input_shape = train_data.shape[1:]\n",
    "    model = build_cnn_model(input_shape)\n",
    "    \n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_data, train_labels, \n",
    "        epochs=epochs, batch_size=batch_size, \n",
    "        validation_data=(test_data, test_labels),\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    predictions = model.predict(test_data).flatten()\n",
    "    pred_labels = (predictions > 0.5).astype(int)\n",
    "    \n",
    "    val_accuracy = np.mean(pred_labels == test_labels)\n",
    "    val_f1 = f1_score(test_labels, pred_labels)\n",
    "    val_auc = roc_auc_score(test_labels, predictions)\n",
    "    val_precision = precision_score(test_labels, pred_labels)\n",
    "    val_recall = recall_score(test_labels, pred_labels)\n",
    "    \n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "    print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "    print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "    print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Load Data\n",
    "data_dir = \"/Users/danigeorge/Documents/HPE Project/malnet-graphs-tiny\"\n",
    "label_dir = \"/Users/danigeorge/Documents/HPE Project/malnet-labels\"\n",
    "graphs, labels = load_graphs_from_edgelist(data_dir, label_dir)\n",
    "train_data, test_data, train_labels, test_labels = create_datasets(graphs, labels)\n",
    "\n",
    "# Train Model\n",
    "model, history = train_model(train_data, train_labels, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd0808f-e2ba-4ce1-aa7e-11894b28d53e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 2.16",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
